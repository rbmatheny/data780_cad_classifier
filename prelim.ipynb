{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac72726b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-15 18:48:35.002184: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-11-15 18:48:35.032842: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-11-15 18:48:35.700588: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b790d7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_surface_lines_np(brep_str):\n",
    "    lines = brep_str.strip().split('\\n')\n",
    "    surface_stop = 0\n",
    "    surface_line = 0\n",
    "\n",
    "    for i, line in enumerate(lines):\n",
    "        if line.startswith('Surface'):\n",
    "            surface_line = i\n",
    "        if line.startswith('Triangulations'):\n",
    "            surface_stop = i\n",
    "            break\n",
    "\n",
    "    surfaces = []\n",
    "\n",
    "    for i in range(surface_line + 1, surface_stop):\n",
    "        surfaces.append(lines[i].strip().split(' '))\n",
    "\n",
    "    surfaces_flat = []\n",
    "    for i in surfaces:\n",
    "        for j in i:\n",
    "            surfaces_flat.append(float(j))\n",
    "\n",
    "    return np.array(surfaces_flat, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8a9308c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 3.          0.          0.         20.          0.          0.\n",
      " -1.         -1.          0.         -0.          0.          1.\n",
      "  0.          0.          0.24497867  1.          0.          0.\n",
      "  0.          0.          0.         -1.         -1.          0.\n",
      " -0.          0.          1.          0.        ]\n"
     ]
    }
   ],
   "source": [
    "# test function\n",
    "with open('models/cone/a cone with a 10.00mm diameter and a 20.00mm height.brep', 'r', encoding='utf-8') as f:\n",
    "    b = f.read()\n",
    "    print(extract_surface_lines_np(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a6f72c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "shapes = ['cone', 'cube', 'cylinder', 'sphere']\n",
    "\n",
    "file_list = [os.listdir('models/' + shape) for shape in shapes]\n",
    "\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "for shape in shapes:\n",
    "    y.append([shape]*len(os.listdir('models/' + shape)))\n",
    "\n",
    "# flatten\n",
    "file_list = [item for items in file_list for item in items]\n",
    "y = [item for items in y for item in items]\n",
    "\n",
    "for i in range(0, len(file_list)):\n",
    "    with open('models/' + y[i] + '/' + file_list[i], 'r', encoding='utf-8') as f:\n",
    "        brep_str = f.read()\n",
    "        X.append(extract_surface_lines_np(brep_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b0ed3cdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "W0000 00:00:1763250516.437460  237955 gpu_device.cc:2342] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    }
   ],
   "source": [
    "from numpy import float32\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "maxlen = 1024\n",
    "X_pad = pad_sequences(X, maxlen=maxlen, dtype='float32', padding='post', truncating='post', value=0.0)\n",
    "X_pad = X_pad[..., None]\n",
    "\n",
    "le = LabelEncoder()\n",
    "y_encoded = le.fit_transform(y)\n",
    "y_arr = np.asarray(y_encoded, dtype='int32')\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_pad, y_arr, test_size=0.2, random_state=3)\n",
    "\n",
    "model = models.Sequential([\n",
    "    layers.Input(shape=(maxlen, 1)),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dense(4, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "45c2571f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.4583 - loss: 1.4889 - val_accuracy: 0.8125 - val_loss: 0.5945\n",
      "Epoch 2/10\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7569 - loss: 0.5251 - val_accuracy: 1.0000 - val_loss: 0.2519\n",
      "Epoch 3/10\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 0.2315 - val_accuracy: 1.0000 - val_loss: 0.1167\n",
      "Epoch 4/10\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 0.1130 - val_accuracy: 1.0000 - val_loss: 0.0582\n",
      "Epoch 5/10\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 0.0595 - val_accuracy: 1.0000 - val_loss: 0.0325\n",
      "Epoch 6/10\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 0.0357 - val_accuracy: 1.0000 - val_loss: 0.0206\n",
      "Epoch 7/10\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 0.0241 - val_accuracy: 1.0000 - val_loss: 0.0145\n",
      "Epoch 8/10\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 0.0179 - val_accuracy: 1.0000 - val_loss: 0.0111\n",
      "Epoch 9/10\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 0.0143 - val_accuracy: 1.0000 - val_loss: 0.0089\n",
      "Epoch 10/10\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 0.0118 - val_accuracy: 1.0000 - val_loss: 0.0075\n"
     ]
    }
   ],
   "source": [
    "model_trained = model.fit(X_train, y_train, epochs=10, validation_split=0.1, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c6565d",
   "metadata": {},
   "source": [
    "# n-dim tensor?\n",
    "# standard scaler?\n",
    "# dense or convolutional?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a4757d3",
   "metadata": {},
   "source": [
    "\n",
    "Right now we have a working baseline model on the CAD shape data that Ross set up. Our focus is on how we check if these models are actually good,  before Confirming  “100% accuracy” without context.\n",
    "\n",
    "We are keeping it simple and consistent: 80% of the data for training, 20% for testing. That way every new model we try can be compared under the same setup.\n",
    "\n",
    "Here’s what were using to evaluate:\n",
    "\n",
    "1. **Overall accuracy** – quick check of how many test shapes we got right. Good for sanity.\n",
    "2. **Per-class accuracy** – how well we do on each shape type (cone, cube, cylinder, sphere) so we can see if the model is only good on one class.\n",
    "3. **Confusion matrix** – to see which classes get mixed up with which. If two shapes are always confused, that tells us something about the features/model.\n",
    "\n",
    "As we add more models or features, we can also plug in things like precision/recall or F1 if we see imbalance or want a fairer view across classes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data780",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
